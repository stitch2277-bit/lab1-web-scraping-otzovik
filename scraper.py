import os
import time
import logging
import random
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class OtzovikScraper:
    """–ü–∞—Ä—Å–µ—Ä –æ—Ç–∑—ã–≤–æ–≤ —Å —Å–∞–π—Ç–∞ otzovik.com"""
    
    def __init__(self, base_url, output_dir='dataset', pages_per_rating=13):
        self.base_url = base_url
        self.output_dir = output_dir
        self.pages_per_rating = pages_per_rating
        self.stats = {
            'total_reviews': 0,
            'saved_reviews': 0,
            'errors': 0
        }
    
    def create_directory_structure(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–∞–ø–æ–∫ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç–∑—ã–≤–æ–≤"""
        try:
            if not os.path.exists(self.output_dir):
                os.makedirs(self.output_dir)
                logger.info("‚úì –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞: %s", self.output_dir)
            
            for rating in range(1, 6):
                rating_dir = os.path.join(self.output_dir, str(rating))
                if not os.path.exists(rating_dir):
                    os.makedirs(rating_dir)
                    logger.info("‚úì –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞: %s", rating_dir)
            
            logger.info("‚úì –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–∞–ø–æ–∫ —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return True
        
        except Exception as e:
            logger.error("‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–∞–ø–æ–∫: %s", e)
            return False
    
    def fetch_page(self, url, attempt=1, max_attempts=3):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        logger.info("–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: %s (–ø–æ–ø—ã—Ç–∫–∞ %d/%d)", url, attempt, max_attempts)
        
        if attempt > max_attempts:
            logger.error("‚úó –ü—Ä–µ–≤—ã—à–µ–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫")
            return None
        
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Safari/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0'
        ]
        
        headers = {
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        try:
            delay = random.uniform(10.0, 12.0)
            logger.info("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ %.1f —Å–µ–∫—É–Ω–¥...", delay)
            time.sleep(delay)
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            if 'captcha' in response.text.lower() or '–¥–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω' in response.text.lower():
                logger.warning("‚ö† –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∑–∞—â–∏—Ç–∞ –æ—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞")
                if attempt < max_attempts:
                    time.sleep(15)
                    return self.fetch_page(url, attempt + 1, max_attempts)
                return None
            
            logger.info("‚úì –°—Ç—Ä–∞–Ω–∏—Ü–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return response.text
        
        except requests.exceptions.HTTPError as e:
            logger.error("‚úó HTTP –æ—à–∏–±–∫–∞ %s: %s", e.response.status_code, e)
            return None
        
        except requests.exceptions.Timeout:
            logger.warning("‚ö† –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞")
            if attempt < max_attempts:
                time.sleep(5)
                return self.fetch_page(url, attempt + 1, max_attempts)
            return None
        
        except requests.exceptions.RequestException as e:
            logger.error("‚úó –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: %s", e)
            return None
    
    def parse_reviews_from_page(self, html):
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–æ–≤ –∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        if not html:
            logger.error("‚úó –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
            return []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            reviews = []
            
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –±–ª–æ–∫–∏ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ –∫–ª–∞—Å—Å–∞–º
            review_blocks = soup.find_all('div', class_=['item status4', 'item status10'])
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
            if not review_blocks:
                all_items = soup.find_all('div', class_='item')
                review_blocks = [item for item in all_items if 'status4' in item.get('class', []) or 'status10' in item.get('class', [])]
            
            logger.info("–ù–∞–π–¥–µ–Ω–æ –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: %d", len(review_blocks))
            
            for block in review_blocks:
                try:
                    # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–π—Ç–∏–Ω–≥
                    rating_elem = block.find('div', class_='rating-score')
                    if rating_elem:
                        rating_text = rating_elem.get_text(strip=True)
                        rating = int(''.join(filter(str.isdigit, rating_text))) if rating_text else None
                    else:
                        rating = None
                    
                    # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                    title_elem = block.find('a', class_='review-title')
                    title = title_elem.get_text(strip=True) if title_elem else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
                    
                    # –ü–æ–ª—É—á–∞–µ–º –∫—Ä–∞—Ç–∫–∏–π —Ç–µ–∫—Å—Ç
                    teaser_elem = block.find('div', class_='review-teaser')
                    teaser = teaser_elem.get_text(strip=True) if teaser_elem else "–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"
                    
                    # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É
                    date_elem = block.find('div', class_='review-postdate')
                    date = date_elem.get_text(strip=True) if date_elem else "–î–∞—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
                    
                    # –ü–æ–ª—É—á–∞–µ–º –∞–≤—Ç–æ—Ä–∞
                    author_elem = block.find('span', itemprop='name')
                    author = author_elem.get_text(strip=True) if author_elem else "–ê–Ω–æ–Ω–∏–º"
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –ø–æ–ª–Ω—ã–π –æ—Ç–∑—ã–≤
                    link_elem = block.find('a', class_='review-title')
                    link = link_elem['href'] if link_elem and 'href' in link_elem.attrs else ""
                    
                    if link and not link.startswith('http'):
                        link = f"https://otzovik.com{link}"
                    
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤–∞
                    review_data = {
                        'rating': rating,
                        'title': title,
                        'teaser': teaser,
                        'date': date,
                        'author': author,
                        'link': link,
                        'full_text': None
                    }
                    
                    reviews.append(review_data)
                    logger.debug("‚úì –û—Ç–∑—ã–≤: %s... (–†–µ–π—Ç–∏–Ω–≥: %s)", title[:30], rating)
                    
                except Exception as e:
                    logger.warning("‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –æ—Ç–∑—ã–≤–∞: %s", e)
                    self.stats['errors'] += 1
                    continue
            
            logger.info("–£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ %d –æ—Ç–∑—ã–≤–æ–≤", len(reviews))
            return reviews
        
        except Exception as e:
            logger.error("‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: %s", e)
            return []
    
    def save_review_to_file(self, review, rating, review_number):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–∞ –≤ —Ñ–∞–π–ª —Å –≤–µ–¥—É—â–∏–º–∏ –Ω—É–ª—è–º–∏"""
        if not isinstance(rating, int) or rating < 1 or rating > 5:
            logger.warning("‚ö† –ü—Ä–æ–ø—É—â–µ–Ω –æ—Ç–∑—ã–≤ #%d —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º —Ä–µ–π—Ç–∏–Ω–≥–æ–º: %s", review_number, rating)
            return False
        
        filename = f"{review_number:04d}.txt"
        filepath = os.path.join(self.output_dir, str(rating), filename)
        
        try:
            # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞ –Ω–µ –±—É–¥–µ—Ç None
            text_to_save = review.get('full_text') or review.get('teaser') or "–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"
            title = review.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
            author = review.get('author', '–ê–Ω–æ–Ω–∏–º')
            date = review.get('date', '–î–∞—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞')
            link = review.get('link', '')
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write("="*60 + "\n")
                f.write(f"–ó–ê–ì–û–õ–û–í–û–ö: {title}\n")
                f.write("="*60 + "\n\n")
                f.write(f"–ê–≤—Ç–æ—Ä: {author}\n")
                f.write(f"–î–∞—Ç–∞: {date}\n")
                f.write(f"–†–µ–π—Ç–∏–Ω–≥: {rating} –∑–≤–µ–∑–¥(—ã)\n")
                if link:
                    f.write(f"–°—Å—ã–ª–∫–∞: {link}\n")
                f.write("\n" + "-"*60 + "\n")
                f.write("–¢–ï–ö–°–¢ –û–¢–ó–´–í–ê:\n")
                f.write("-"*60 + "\n\n")
                f.write(text_to_save + "\n\n")
                f.write("="*60 + "\n")
                f.write(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("="*60 + "\n")
            
            self.stats['saved_reviews'] += 1
            logger.info("‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω: %s", filepath)
            return True
        
        except PermissionError:
            logger.error("‚úó –û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ —Ñ–∞–π–ª—É: %s", filepath)
            self.stats['errors'] += 1
            return False
        
        except Exception as e:
            logger.error("‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ %s: %s", filepath, e)
            self.stats['errors'] += 1
            return False
    
    def scrape_reviews_by_rating(self, rating_value):
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–æ–≤ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ —Ä–µ–π—Ç–∏–Ω–≥–∞"""
        logger.info("\n" + "="*60)
        logger.info("–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–æ–≤ —Å —Ä–µ–π—Ç–∏–Ω–≥–æ–º: %d –∑–≤–µ–∑–¥", rating_value)
        logger.info("="*60 + "\n")
        
        all_reviews = []
        
        for page_num in tqdm(range(1, self.pages_per_rating + 1), desc=f"–†–µ–π—Ç–∏–Ω–≥ {rating_value}", unit="—Å—Ç—Ä"):
            if page_num == 1:
                url = f"{self.base_url}?ratio={rating_value}"
            else:
                url = f"{self.base_url}{page_num}/?ratio={rating_value}"
            
            logger.info("üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ %d/%d: %s", page_num, self.pages_per_rating, url)
            
            html = self.fetch_page(url)
            
            if html:
                reviews = self.parse_reviews_from_page(html)
                all_reviews.extend(reviews)
                self.stats['total_reviews'] += len(reviews)
            
            if page_num < self.pages_per_rating:
                time.sleep(random.uniform(8.0, 10.0))
        
        return all_reviews
    
    def run(self):
        """–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        logger.info("="*60)
        logger.info("Web-Scraper –¥–ª—è otzovik.com")
        logger.info("–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–æ–≤ –æ –°–±–µ—Ä–±–∞–Ω–∫–µ")
        logger.info("="*60)
        
        if not self.create_directory_structure():
            logger.error("‚úó –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫")
            return
        
        all_reviews_by_rating = {}
        
        for rating in range(1, 6):
            reviews = self.scrape_reviews_by_rating(rating)
            all_reviews_by_rating[rating] = reviews
            logger.info("üìä –†–µ–π—Ç–∏–Ω–≥ %d: —Å–æ–±—Ä–∞–Ω–æ %d –æ—Ç–∑—ã–≤–æ–≤", rating, len(reviews))
        
        total_collected = sum(len(reviews) for reviews in all_reviews_by_rating.values())
        logger.info("\nüìä –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ –æ—Ç–∑—ã–≤–æ–≤: %d", total_collected)
        
        logger.info("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤ –≤ —Ñ–∞–π–ª—ã...")
        
        for rating, reviews in all_reviews_by_rating.items():
            logger.info("\n–†–µ–π—Ç–∏–Ω–≥ %d –∑–≤–µ–∑–¥ - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ...", rating)
            
            for i, review in enumerate(tqdm(reviews, desc=f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {rating}"), start=1):
                self.save_review_to_file(review, rating, i)
        
        logger.info("\n" + "="*60)
        logger.info("‚úÖ –ó–ê–í–ï–†–®–ï–ù–û!")
        logger.info("="*60)
        logger.info("üìÅ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫—É: %s/", self.output_dir)
        logger.info("üìÑ –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: %d –æ—Ç–∑—ã–≤–æ–≤", self.stats['total_reviews'])
        logger.info("üìÑ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: %d –æ—Ç–∑—ã–≤–æ–≤", self.stats['saved_reviews'])
        logger.info("üìÑ –û—à–∏–±–æ–∫: %d", self.stats['errors'])
        if self.stats['total_reviews'] > 0:
            success_rate = (self.stats['saved_reviews'] / self.stats['total_reviews']) * 100
            logger.info("üìä –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: %.1f%%", success_rate)
        logger.info("üìÑ –õ–æ–≥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: scraper.log")
        logger.info("="*60)


def main():
    base_url = "https://otzovik.com/reviews/sberbank_rossii/"
    
    scraper = OtzovikScraper(
        base_url=base_url,
        output_dir='dataset',
        pages_per_rating=13  # 13 —Å—Ç—Ä–∞–Ω–∏—Ü √ó ~40 –æ—Ç–∑—ã–≤–æ–≤ = ~520 –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ –∫–∞–∂–¥—ã–π —Ä–µ–π—Ç–∏–Ω–≥
    )
    
    scraper.run()


if __name__ == "__main__":
    main()