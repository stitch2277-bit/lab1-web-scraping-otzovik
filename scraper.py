import os
import time
import logging
import random
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def create_directory_structure(base_dir='dataset'):
    try:
        if not os.path.exists(base_dir):
            os.makedirs(base_dir)
            logger.info("‚úì –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞: %s", base_dir)
        
        for rating in range(1, 6):
            rating_dir = os.path.join(base_dir, str(rating))
            if not os.path.exists(rating_dir):
                os.makedirs(rating_dir)
                logger.info("‚úì –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞: %s", rating_dir)
        
        logger.info("‚úì –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–∞–ø–æ–∫ —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        return True
    
    except Exception as e:
        logger.error("‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–∞–ø–æ–∫: %s", e)
        return False

def fetch_page_with_requests(url, attempt=1, max_attempts=3):
    logger.info("–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: %s (–ø–æ–ø—ã—Ç–∫–∞ %d/%d)", url, attempt, max_attempts)
    
    if attempt > max_attempts:
        logger.error("‚úó –ü—Ä–µ–≤—ã—à–µ–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫")
        return None
    
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Safari/605.1.15',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'
    ]
    
    headers = {
        'User-Agent': random.choice(user_agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    try:
        delay = random.uniform(10.0, 12.0)
        logger.info(" –û–∂–∏–¥–∞–Ω–∏–µ %.1f —Å–µ–∫—É–Ω–¥...", delay)
        time.sleep(delay)
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        if 'captcha' in response.text.lower() or '–¥–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω' in response.text.lower():
            logger.warning("‚ö† –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∑–∞—â–∏—Ç–∞ –æ—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞")
            if attempt < max_attempts:
                time.sleep(15)
                return fetch_page_with_requests(url, attempt + 1, max_attempts)
            return None
        
        logger.info("‚úì –°—Ç—Ä–∞–Ω–∏—Ü–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        return response.text
    
    except requests.exceptions.HTTPError as e:
        logger.error("‚úó HTTP –æ—à–∏–±–∫–∞ %s: %s", e.response.status_code, e)
        return None
    
    except requests.exceptions.Timeout:
        logger.warning("‚ö† –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞")
        if attempt < max_attempts:
            time.sleep(5)
            return fetch_page_with_requests(url, attempt + 1, max_attempts)
        return None
    
    except requests.exceptions.RequestException as e:
        logger.error("‚úó –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: %s", e)
        return None

def fetch_full_review_text(review_url):
    try:
        html = fetch_page_with_requests(review_url)
        
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        review_body = soup.find('div', class_='review-body')
        
        if not review_body:
            review_body = soup.find('div', class_='review-full')
        
        if not review_body:
            review_body = soup.find('div', itemprop='reviewBody')
        
        if review_body:
            for elem in review_body.find_all(['script', 'style', 'button', 'a']):
                elem.decompose()
            
            full_text = review_body.get_text(separator='\n', strip=True)
            return full_text
        
        return None
    
    except Exception as e:
        logger.warning("‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: %s", e)
        return None

def parse_reviews(html):
    if not html:
        logger.error("‚úó –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
        return []
    
    try:
        soup = BeautifulSoup(html, 'html.parser')
        reviews = []
        
        all_items = soup.find_all('div', class_='item')
        review_blocks = [item for item in all_items if 'status4' in item.get('class', []) or 'status10' in item.get('class', [])]
        
        logger.info("–ù–∞–π–¥–µ–Ω–æ –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: %d", len(review_blocks))
        
        for block in review_blocks[:10]:
            try:
                rating_meta = block.find('meta', itemprop='reviewRating')
                if rating_meta and rating_meta.get('content'):
                    rating = int(float(rating_meta['content']))
                else:
                    rating_elem = block.find('div', class_='rating-score')
                    if rating_elem:
                        rating_text = rating_elem.get_text(strip=True)
                        rating = int(''.join(filter(str.isdigit, rating_text))) if rating_text else None
                    else:
                        rating = None
                
                title_elem = block.find('a', class_='review-title')
                title = title_elem.get_text(strip=True) if title_elem else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
                
                link_elem = block.find('a', class_='review-title')
                link = link_elem['href'] if link_elem and 'href' in link_elem.attrs else ""
                
                if link and not link.startswith('http'):
                    link = f"https://otzovik.com{link}"
                
                teaser_elem = block.find('div', class_='review-teaser')
                teaser = teaser_elem.get_text(strip=True) if teaser_elem else "–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"
                
                date_elem = block.find('div', class_='review-postdate')
                date = date_elem.get_text(strip=True) if date_elem else "–î–∞—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
                
                author_elem = block.find('span', itemprop='name')
                author = author_elem.get_text(strip=True) if author_elem else "–ê–Ω–æ–Ω–∏–º"
                
                review_data = {
                    'rating': rating,
                    'title': title,
                    'teaser': teaser,
                    'date': date,
                    'author': author,
                    'link': link,
                    'full_text': None
                }
                
                reviews.append(review_data)
                logger.info("‚úì –û—Ç–∑—ã–≤: %s... (–†–µ–π—Ç–∏–Ω–≥: %s, –ê–≤—Ç–æ—Ä: %s)", title[:30], rating, author)
                
            except Exception as e:
                logger.warning("‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –æ—Ç–∑—ã–≤–∞: %s", e)
                continue
        
        logger.info("–£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ %d –æ—Ç–∑—ã–≤–æ–≤", len(reviews))
        return reviews
    
    except Exception as e:
        logger.error("‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: %s", e)
        return []

def save_review_to_file(review, rating, review_number, base_dir='dataset'):
    if not isinstance(rating, int) or rating < 1 or rating > 5:
        logger.warning("‚ö† –ü—Ä–æ–ø—É—â–µ–Ω –æ—Ç–∑—ã–≤ #%d —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º —Ä–µ–π—Ç–∏–Ω–≥–æ–º: %s", review_number, rating)
        return False
    
    filename = f"{review_number:04d}.txt"
    filepath = os.path.join(base_dir, str(rating), filename)
    
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write("="*60 + "\n")
            f.write(f"–ó–ê–ì–û–õ–û–í–û–ö: {review['title']}\n")
            f.write("="*60 + "\n\n")
            f.write(f"–ê–≤—Ç–æ—Ä: {review['author']}\n")
            f.write(f"–î–∞—Ç–∞: {review['date']}\n")
            f.write(f"–†–µ–π—Ç–∏–Ω–≥: {review['rating']} –∑–≤–µ–∑–¥(—ã)\n")
            f.write(f"–°—Å—ã–ª–∫–∞: {review['link']}\n\n")
            f.write("-"*60 + "\n")
            f.write("–¢–ï–ö–°–¢ –û–¢–ó–´–í–ê:\n")
            f.write("-"*60 + "\n\n")
            
            text_to_save = review.get('full_text', review['teaser'])
            f.write(text_to_save + "\n\n")
            
            f.write("="*60 + "\n")
            f.write(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*60 + "\n")
        
        logger.info("‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω: %s", filepath)
        return True
    
    except PermissionError:
        logger.error("‚úó –û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ —Ñ–∞–π–ª—É: %s", filepath)
        return False
    
    except Exception as e:
        logger.error("‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ %s: %s", filepath, e)
        return False

def main():
    logger.info("="*60)
    logger.info("Web-Scraper –¥–ª—è otzovik.com")
    logger.info("–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–æ–≤ –æ –°–±–µ—Ä–±–∞–Ω–∫–µ")
    logger.info("="*60)
    
    if not create_directory_structure():
        logger.error("‚úó –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫")
        return
    
    base_url = "https://otzovik.com/reviews/sberbank_rossii/"
    
    logger.info("\n –¶–µ–ª–µ–≤–æ–π —Å–∞–π—Ç: %s", base_url)
    
    html = fetch_page_with_requests(base_url)
    
    if not html:
        logger.error("‚úó –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å —Å–∞–π—Ç–∞")
        return
    
    reviews = parse_reviews(html)
    
    if not reviews:
        logger.error("‚úó –û—Ç–∑—ã–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return
    
    logger.info("\n –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –æ—Ç–∑—ã–≤–æ–≤: %d", len(reviews))
    
    logger.info("\n –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –æ—Ç–∑—ã–≤–æ–≤...")
    
    for i, review in enumerate(tqdm(reviews, desc="–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤"), start=1):
        if review['link']:
            full_text = fetch_full_review_text(review['link'])
            if full_text:
                review['full_text'] = full_text
                logger.debug("‚úì –ü–æ–ª—É—á–µ–Ω –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–∑—ã–≤–∞ %d", i)
        
        if i < len(reviews):
            time.sleep(random.uniform(6.0, 8.0))
    
    saved_count = 0
    logger.info("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤ –≤ —Ñ–∞–π–ª—ã...")
    
    for i, review in enumerate(tqdm(reviews, desc="–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤"), start=1):
        if save_review_to_file(review, review['rating'], i):
            saved_count += 1
    
    logger.info("="*60)
    logger.info(" –ó–ê–í–ï–†–®–ï–ù–û!")
    logger.info(" –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫—É: dataset/")
    logger.info(" –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ %d –æ—Ç–∑—ã–≤–æ–≤", saved_count)
    logger.info(" –õ–æ–≥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: scraper.log")
    logger.info("="*60)

if __name__ == "__main__":
    main()